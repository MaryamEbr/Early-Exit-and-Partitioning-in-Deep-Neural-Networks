{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a90ed8f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import add, Input, Dense, Conv2D, MaxPooling2D, AveragePooling2D, ZeroPadding2D, Dropout, Flatten, Concatenate, Reshape, Activation, BatchNormalization, GlobalAveragePooling2D, ZeroPadding2D\n",
    "from tensorflow.nn import local_response_normalization\n",
    "from tensorflow.python.keras.layers.merge import concatenate\n",
    "from tensorflow.keras.activations import relu\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import matplotlib\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.callbacks import LearningRateScheduler\n",
    "from tensorflow.keras.regularizers import l2, l1, l1_l2\n",
    "from itertools import permutations, combinations\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "35b3ddc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device mapping:\n",
      "/job:localhost/replica:0/task:0/device:GPU:0 -> device: 0, name: GeForce RTX 3080, pci bus id: 0000:41:00.0, compute capability: 8.6\n",
      "\n"
     ]
    }
   ],
   "source": [
    "config = tf.compat.v1.ConfigProto()\n",
    "config.gpu_options.allow_growth = True  # dynamically grow the memory used on the GPU\n",
    "config.log_device_placement = True  # to log device placement (on which device the operation ran)\n",
    "# config.gpu_options.per_process_gpu_memory_fraction = 0.60\n",
    "sess = tf.compat.v1.Session(config=config)\n",
    "tf.compat.v1.keras.backend.set_session(sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7d87ed47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 50000 images belonging to 10 classes.\n",
      "Found 5000 images belonging to 10 classes.\n",
      "Found 5000 images belonging to 10 classes.\n",
      "data batch shape: (32, 32, 32, 3)\n",
      "labels batch shape: (32, 10)\n"
     ]
    }
   ],
   "source": [
    "#### Using CIFAR10 dataset\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.cifar10.load_data()\n",
    "\n",
    "# Preprocess the data (these are NumPy arrays)\n",
    "x_train = x_train.astype(\"float32\") / 255\n",
    "x_test = x_test.astype(\"float32\") / 255\n",
    "\n",
    "\n",
    "y_train = y_train.astype(\"float32\")\n",
    "y_test = y_test.astype(\"float32\")\n",
    "\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes=10)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes=10) \n",
    "\n",
    "global_batch_size = 32\n",
    "image_resize = 32\n",
    "\n",
    "########## Train\n",
    "#### removing data augmentation only here for profiling\n",
    "train_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "# train_datagen = ImageDataGenerator(rescale=1./255, featurewise_center = True,\n",
    "#                                    rotation_range = 20, horizontal_flip = True, height_shift_range = 0.2,\n",
    "#                                    width_shift_range = 0.2, zoom_range = 0.2, channel_shift_range = 0.2)\n",
    "\n",
    "\n",
    "train_it = train_datagen.flow_from_directory(\n",
    "        'cifar10/train',\n",
    "        class_mode='categorical',\n",
    "        target_size=(image_resize, image_resize),\n",
    "        batch_size=global_batch_size)\n",
    "\n",
    "\n",
    "############ Test\n",
    "test_datagen = ImageDataGenerator(rescale=1./255, validation_split=0.5)\n",
    "\n",
    "validation_it = test_datagen.flow_from_directory(\n",
    "        'cifar10/test',\n",
    "        class_mode='categorical',\n",
    "        target_size=(image_resize, image_resize),\n",
    "        batch_size=global_batch_size,\n",
    "        subset = \"training\",seed = 545)\n",
    "\n",
    "test_it = test_datagen.flow_from_directory(\n",
    "        'cifar10/test',\n",
    "        class_mode='categorical',\n",
    "        target_size=(image_resize, image_resize),\n",
    "        batch_size=global_batch_size,\n",
    "        subset = \"validation\",\n",
    "        seed = 545)\n",
    "\n",
    "\n",
    "for data_batch, labels_batch in train_it:\n",
    "    print('data batch shape:', data_batch.shape)\n",
    "    print('labels batch shape:', labels_batch.shape)\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fe0f4712",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomLoss(keras.losses.Loss):\n",
    "    def __init__(self, factor):\n",
    "        super().__init__()\n",
    "        self.factor = factor\n",
    "\n",
    "    def call(self, y_true, y_pred):\n",
    "        ce = tf.keras.losses.categorical_crossentropy(y_true, y_pred)\n",
    "        return ce * self.factor\n",
    "    \n",
    "class Residual_block(tf.keras.Model):\n",
    "    def __init__(self, num_channels, use_1x1conv=False, strides=1):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.conv1 = Conv2D(num_channels, kernel_size=3, padding='same', strides=strides, kernel_initializer='he_uniform', kernel_regularizer=l2(0.00001), bias_regularizer=l2(0.00001))\n",
    "        self.conv2 = Conv2D(num_channels, kernel_size=3, padding='same', kernel_initializer='he_uniform', kernel_regularizer=l2(0.00001), bias_regularizer=l2(0.00001))\n",
    "        self.conv3 = None\n",
    "        \n",
    "        if use_1x1conv:\n",
    "            self.conv3 = Conv2D(num_channels, kernel_size=1, strides=strides, kernel_initializer='he_uniform', kernel_regularizer=l2(0.00001), bias_regularizer=l2(0.00001))\n",
    "            \n",
    "        self.bn1 = BatchNormalization()\n",
    "        self.bn2 = BatchNormalization()\n",
    "        \n",
    "    def call(self, X, training):\n",
    "        \n",
    "        if (training != False and training != True):\n",
    "            training = False\n",
    "        \n",
    "        Y = self.conv1(X)\n",
    "        Y = self.bn1 (Y, training = training)\n",
    "        Y = relu(Y)\n",
    "        Y = self.conv2(Y)\n",
    "        Y = self.bn2 (Y, training = training)\n",
    "        \n",
    "        if self.conv3 is not None:\n",
    "            X = self.conv3(X)\n",
    "            \n",
    "        Y += X\n",
    "        Y = relu(Y)\n",
    "        return Y\n",
    "\n",
    "\n",
    "class CIFAR_ResNet(tf.keras.Model):\n",
    "    def __init__(self, model_n, branch_number):\n",
    "        super(CIFAR_ResNet, self).__init__()\n",
    "        \n",
    "        self.model_n = model_n\n",
    "        self.branch_number = branch_number\n",
    "        self.residual_layers = []\n",
    "        \n",
    "        ######## Begining Layers\n",
    "        self.conv1 = Conv2D(16, kernel_size=3, padding='same',kernel_initializer='he_uniform', input_shape=(image_resize, image_resize, 3), kernel_regularizer=l2(0.00001), bias_regularizer=l2(0.00001))\n",
    "        self.bn1 = BatchNormalization()\n",
    "        \n",
    "        \n",
    "        ######## ResNet layers\n",
    "        ### first block, 16 channels\n",
    "        for i in range(self.model_n):\n",
    "            self.residual_layers.append(Residual_block(16))\n",
    "            \n",
    "        ### second block, 32 channels\n",
    "        for i in range(self.model_n):\n",
    "            if i == 0:\n",
    "                self.residual_layers.append(Residual_block(32, use_1x1conv=True, strides=2))\n",
    "            else:\n",
    "                self.residual_layers.append(Residual_block(32))\n",
    "                \n",
    "        ### third block, 64 channels\n",
    "        self.residual_layers_block3 = []\n",
    "        for i in range(self.model_n):\n",
    "            if i == 0:\n",
    "                self.residual_layers.append(Residual_block(64, use_1x1conv=True, strides=2))\n",
    "            else:\n",
    "                self.residual_layers.append(Residual_block(64))\n",
    "\n",
    "        \n",
    "        ######## OUTPUT Layers\n",
    "        self.pool_out = GlobalAveragePooling2D()\n",
    "        self.flat_out = Flatten()\n",
    "\n",
    "        self.dense_layers = []\n",
    "        for i in range(self.branch_number):\n",
    "            self.dense_layers.append(Dense(10, kernel_initializer='he_uniform', kernel_regularizer=l2(0.00001), bias_regularizer=l2(0.00001), activation='softmax'))\n",
    "        \n",
    "\n",
    "    def call(self, inputs, training):\n",
    "        inference_flag = -1000\n",
    "        layer_ind_counter = 0\n",
    "        comp_latency_list_backbone = [0 for i in range(self.branch_number)]\n",
    "        comp_latency_list_exitbranch = [0 for i in range(self.branch_number)]\n",
    "        out_vector_list = [[] for i in range(self.branch_number)]\n",
    "        \n",
    "        if (training != False and training != True):\n",
    "            inference_flag = 1000\n",
    "            training = False\n",
    "            \n",
    "            \n",
    "        ##### Begining layers\n",
    "        start_time = tf.timestamp()\n",
    "        x = self.conv1(inputs)\n",
    "        x = self.bn1 (x, training = training)\n",
    "        x = relu (x)\n",
    "        comp_latency_list_backbone[layer_ind_counter] = tf.timestamp() - start_time\n",
    "        \n",
    "        \n",
    "        # exit branch\n",
    "        start_time = tf.timestamp()\n",
    "        temp = self.pool_out(x)\n",
    "        temp = self.flat_out(temp)\n",
    "        out_vector_list[layer_ind_counter] = self.dense_layers[layer_ind_counter](temp)\n",
    "        comp_latency_list_exitbranch[layer_ind_counter] = tf.timestamp() - start_time\n",
    "        \n",
    "        \n",
    "        \n",
    "        ##### ResNet Layers\n",
    "        for layer in self.residual_layers:\n",
    "            layer_ind_counter += 1\n",
    "            start_time = tf.timestamp()\n",
    "            x = layer (x)\n",
    "            comp_latency_list_backbone[layer_ind_counter] = tf.timestamp() - start_time\n",
    "            \n",
    "            # add exit branch after each resnet block\n",
    "            start_time = tf.timestamp()\n",
    "            temp = self.pool_out(x)\n",
    "            temp = self.flat_out(temp)\n",
    "            out_vector_list[layer_ind_counter] = self.dense_layers[layer_ind_counter](temp)\n",
    "            comp_latency_list_exitbranch[layer_ind_counter] = tf.timestamp() - start_time\n",
    "            \n",
    "\n",
    "\n",
    "        if (inference_flag == 1000):\n",
    "            return(out_vector_list, comp_latency_list_backbone, comp_latency_list_exitbranch)\n",
    "        \n",
    "        \n",
    "        return out_vector_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ecafcb3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "157/157 [==============================] - 5s 19ms/step - loss: 8.5724 - output_1_loss: 1.7556 - output_2_loss: 1.2076 - output_3_loss: 1.0540 - output_4_loss: 1.0171 - output_5_loss: 0.7794 - output_6_loss: 0.5720 - output_7_loss: 0.5007 - output_8_loss: 0.4383 - output_9_loss: 0.3808 - output_10_loss: 0.3652 - output_1_accuracy: 0.3992 - output_2_accuracy: 0.5918 - output_3_accuracy: 0.6510 - output_4_accuracy: 0.6742 - output_5_accuracy: 0.7652 - output_6_accuracy: 0.8232 - output_7_accuracy: 0.8432 - output_8_accuracy: 0.8662 - output_9_accuracy: 0.8790 - output_10_accuracy: 0.8848\n"
     ]
    }
   ],
   "source": [
    "##### ResNet20 + CIFAR10(32x32)\n",
    "\n",
    "##### load weight\n",
    "opt = tf.keras.optimizers.SGD(momentum=0.9)\n",
    "\n",
    "##### for ResNet20\n",
    "branch_number= 10\n",
    "model_n = 3\n",
    "\n",
    "ls = [CustomLoss(1) for i in range(branch_number)]\n",
    "\n",
    "model_CIFAR_ResNet = CIFAR_ResNet(model_n,branch_number)\n",
    "model_CIFAR_ResNet.compile(optimizer=opt, loss=ls, metrics=['accuracy'])\n",
    "model_CIFAR_ResNet.load_weights('profiling_models/ResNet20_10out_bs32_epoch800_lr0.01_new')\n",
    "model_CIFAR_ResNet.evaluate(test_it);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d45fce3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "157/157 [==============================] - 6s 23ms/step - loss: 9.8817 - output_1_loss: 1.6978 - output_2_loss: 1.3324 - output_3_loss: 1.2345 - output_4_loss: 1.1332 - output_5_loss: 1.0083 - output_6_loss: 0.8086 - output_7_loss: 0.7023 - output_8_loss: 0.5606 - output_9_loss: 0.4608 - output_10_loss: 0.4325 - output_1_accuracy: 0.3832 - output_2_accuracy: 0.5372 - output_3_accuracy: 0.5820 - output_4_accuracy: 0.6084 - output_5_accuracy: 0.6640 - output_6_accuracy: 0.7296 - output_7_accuracy: 0.7618 - output_8_accuracy: 0.8222 - output_9_accuracy: 0.8596 - output_10_accuracy: 0.8668\n"
     ]
    }
   ],
   "source": [
    "##### ResNet20 + CIFAR10(128x128)\n",
    "\n",
    "##### load weight\n",
    "opt = tf.keras.optimizers.SGD(momentum=0.9)\n",
    "\n",
    "##### for ResNet20\n",
    "branch_number= 10\n",
    "model_n = 3\n",
    "\n",
    "ls = [CustomLoss(1) for i in range(branch_number)]\n",
    "\n",
    "model_CIFAR_ResNet = CIFAR_ResNet(model_n,branch_number)\n",
    "model_CIFAR_ResNet.compile(optimizer=opt, loss=ls, metrics=['accuracy'])\n",
    "model_CIFAR_ResNet.load_weights('profiling_models/ResNet20_10out_bs32_epoch500_lr0.01_128x128')\n",
    "model_CIFAR_ResNet.evaluate(test_it);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "11ca089b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "157/157 [==============================] - 12s 45ms/step - loss: 16.0903 - output_1_loss: 1.7119 - output_2_loss: 1.4054 - output_3_loss: 1.2121 - output_4_loss: 1.0742 - output_5_loss: 0.9618 - output_6_loss: 0.8778 - output_7_loss: 0.7946 - output_8_loss: 0.8157 - output_9_loss: 0.7359 - output_10_loss: 0.5949 - output_11_loss: 0.5141 - output_12_loss: 0.4587 - output_13_loss: 0.4226 - output_14_loss: 0.4055 - output_15_loss: 0.4039 - output_16_loss: 0.3943 - output_17_loss: 0.3567 - output_18_loss: 0.3354 - output_19_loss: 0.3271 - output_20_loss: 0.3258 - output_21_loss: 0.3224 - output_22_loss: 0.3224 - output_1_accuracy: 0.3992 - output_2_accuracy: 0.5186 - output_3_accuracy: 0.5884 - output_4_accuracy: 0.6542 - output_5_accuracy: 0.6904 - output_6_accuracy: 0.7264 - output_7_accuracy: 0.7536 - output_8_accuracy: 0.7466 - output_9_accuracy: 0.7704 - output_10_accuracy: 0.8142 - output_11_accuracy: 0.8436 - output_12_accuracy: 0.8582 - output_13_accuracy: 0.8702 - output_14_accuracy: 0.8754 - output_15_accuracy: 0.8750 - output_16_accuracy: 0.8766 - output_17_accuracy: 0.8856 - output_18_accuracy: 0.8934 - output_19_accuracy: 0.8932 - output_20_accuracy: 0.8952 - output_21_accuracy: 0.8962 - output_22_accuracy: 0.8952\n"
     ]
    }
   ],
   "source": [
    "##### ResNet44 + CIFAR10(32x32)\n",
    "\n",
    "##### load weight\n",
    "opt = tf.keras.optimizers.SGD(momentum=0.9)\n",
    "\n",
    "##### for ResNet44\n",
    "branch_number= 22\n",
    "model_n = 7\n",
    "\n",
    "ls = [CustomLoss(1) for i in range(branch_number)]\n",
    "\n",
    "model_CIFAR_ResNet = CIFAR_ResNet(model_n,branch_number)\n",
    "model_CIFAR_ResNet.compile(optimizer=opt, loss=ls, metrics=['accuracy'])\n",
    "model_CIFAR_ResNet.load_weights('profiling_models/ResNet44_22out_bs32_epoch500_lr0.01')\n",
    "model_CIFAR_ResNet.evaluate(test_it);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e3695952",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "157/157 [==============================] - 32s 82ms/step - loss: 35.5343 - output_1_loss: 1.8200 - output_2_loss: 1.5343 - output_3_loss: 1.5059 - output_4_loss: 1.3007 - output_5_loss: 1.1630 - output_6_loss: 1.0567 - output_7_loss: 1.0033 - output_8_loss: 0.9489 - output_9_loss: 0.9098 - output_10_loss: 0.8414 - output_11_loss: 0.8184 - output_12_loss: 0.7825 - output_13_loss: 0.7167 - output_14_loss: 0.6884 - output_15_loss: 0.6591 - output_16_loss: 0.6486 - output_17_loss: 0.6587 - output_18_loss: 0.6652 - output_19_loss: 0.7007 - output_20_loss: 0.6712 - output_21_loss: 0.6390 - output_22_loss: 0.5755 - output_23_loss: 0.5093 - output_24_loss: 0.4625 - output_25_loss: 0.4157 - output_26_loss: 0.3866 - output_27_loss: 0.3581 - output_28_loss: 0.3449 - output_29_loss: 0.3389 - output_30_loss: 0.3296 - output_31_loss: 0.3224 - output_32_loss: 0.3201 - output_33_loss: 0.3181 - output_34_loss: 0.3149 - output_35_loss: 0.3146 - output_36_loss: 0.3146 - output_37_loss: 0.3206 - output_38_loss: 0.3260 - output_39_loss: 0.3160 - output_40_loss: 0.2980 - output_41_loss: 0.2952 - output_42_loss: 0.2900 - output_43_loss: 0.2919 - output_44_loss: 0.2903 - output_45_loss: 0.2889 - output_46_loss: 0.2903 - output_47_loss: 0.2891 - output_48_loss: 0.2895 - output_49_loss: 0.2880 - output_50_loss: 0.2891 - output_51_loss: 0.2885 - output_52_loss: 0.2882 - output_53_loss: 0.2878 - output_54_loss: 0.2878 - output_55_loss: 0.2885 - output_1_accuracy: 0.3432 - output_2_accuracy: 0.4572 - output_3_accuracy: 0.4890 - output_4_accuracy: 0.5578 - output_5_accuracy: 0.6104 - output_6_accuracy: 0.6520 - output_7_accuracy: 0.6762 - output_8_accuracy: 0.6906 - output_9_accuracy: 0.7074 - output_10_accuracy: 0.7338 - output_11_accuracy: 0.7468 - output_12_accuracy: 0.7598 - output_13_accuracy: 0.7812 - output_14_accuracy: 0.7922 - output_15_accuracy: 0.7996 - output_16_accuracy: 0.8048 - output_17_accuracy: 0.8006 - output_18_accuracy: 0.8000 - output_19_accuracy: 0.7884 - output_20_accuracy: 0.8070 - output_21_accuracy: 0.8140 - output_22_accuracy: 0.8306 - output_23_accuracy: 0.8482 - output_24_accuracy: 0.8660 - output_25_accuracy: 0.8764 - output_26_accuracy: 0.8794 - output_27_accuracy: 0.8886 - output_28_accuracy: 0.8934 - output_29_accuracy: 0.8986 - output_30_accuracy: 0.9046 - output_31_accuracy: 0.9068 - output_32_accuracy: 0.9072 - output_33_accuracy: 0.9060 - output_34_accuracy: 0.9074 - output_35_accuracy: 0.9064 - output_36_accuracy: 0.9064 - output_37_accuracy: 0.9020 - output_38_accuracy: 0.9076 - output_39_accuracy: 0.9048 - output_40_accuracy: 0.9080 - output_41_accuracy: 0.9112 - output_42_accuracy: 0.9136 - output_43_accuracy: 0.9134 - output_44_accuracy: 0.9138 - output_45_accuracy: 0.9140 - output_46_accuracy: 0.9132 - output_47_accuracy: 0.9144 - output_48_accuracy: 0.9138 - output_49_accuracy: 0.9148 - output_50_accuracy: 0.9132 - output_51_accuracy: 0.9136 - output_52_accuracy: 0.9154 - output_53_accuracy: 0.9146 - output_54_accuracy: 0.9148 - output_55_accuracy: 0.9144\n"
     ]
    }
   ],
   "source": [
    "##### ResNet110 + CIFAR10(32x32)\n",
    "\n",
    "##### load weight\n",
    "opt = tf.keras.optimizers.SGD(momentum=0.9)\n",
    "\n",
    "##### for ResNet110\n",
    "branch_number= 55\n",
    "model_n = 18\n",
    "\n",
    "ls = [CustomLoss(1) for i in range(branch_number)]\n",
    "\n",
    "model_CIFAR_ResNet = CIFAR_ResNet(model_n,branch_number)\n",
    "model_CIFAR_ResNet.compile(optimizer=opt, loss=ls, metrics=['accuracy'])\n",
    "model_CIFAR_ResNet.load_weights('profiling_models/ResNet110_55out_bs32_epoch800_lr0.01')\n",
    "model_CIFAR_ResNet.evaluate(test_it);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b244f6c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "###### now the profiling part\n",
    "###### TRAIN PART"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f6d5967e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "%%%%%%%%%%%%%%%%%%%%%%% ResNet20 + CIFAR10(32x32)%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "computation latencies backbone(train data)    [1.06119835 1.56263262 1.35406359 1.35417639 1.75180006 1.41749927\n",
      " 1.44090811 1.64432199 1.35139916 1.37869434]\n",
      "computation latencies exitbranch(train data)    [0.57011324 0.57846146 0.58294255 0.57056284 0.57336909 0.564956\n",
      " 0.56142836 0.56560254 0.56013978 0.55872234]\n",
      "&&&&& selected exit  [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]   &&&&&&\n",
      "------------------  1e-09 ------------------------\n",
      "exit rate per thresh  {6: 14, 7: 23, 8: 40, 9: 123, 10: 49800}\n",
      "------------------  0.052631527263157896 ------------------------\n",
      "exit rate per thresh  {1: 802, 2: 3435, 3: 4150, 4: 2270, 5: 7682, 6: 7863, 7: 3649, 8: 4254, 9: 3258, 10: 12637}\n",
      "------------------  0.10526305352631579 ------------------------\n",
      "exit rate per thresh  {1: 1479, 2: 5763, 3: 5648, 4: 2715, 5: 8320, 6: 7636, 7: 3325, 8: 3619, 9: 2731, 10: 8764}\n",
      "------------------  0.1578945797894737 ------------------------\n",
      "exit rate per thresh  {1: 2167, 2: 8004, 3: 6311, 4: 3064, 5: 8572, 6: 7129, 7: 2978, 8: 3130, 9: 2259, 10: 6386}\n",
      "------------------  0.21052610605263158 ------------------------\n",
      "exit rate per thresh  {1: 2829, 2: 10225, 3: 6822, 4: 3283, 5: 8608, 6: 6468, 7: 2674, 8: 2687, 9: 1806, 10: 4598}\n",
      "------------------  0.26315763231578954 ------------------------\n",
      "exit rate per thresh  {1: 3618, 2: 12477, 3: 7215, 4: 3481, 5: 8299, 6: 5794, 7: 2328, 8: 2141, 9: 1386, 10: 3261}\n",
      "------------------  0.3157891585789474 ------------------------\n",
      "exit rate per thresh  {1: 4859, 2: 15021, 3: 7301, 4: 3399, 5: 7732, 6: 5053, 7: 1880, 8: 1631, 9: 1045, 10: 2079}\n",
      "------------------  0.3684206848421053 ------------------------\n",
      "exit rate per thresh  {1: 6685, 2: 16695, 3: 7441, 4: 3320, 5: 6991, 6: 4132, 7: 1472, 8: 1257, 9: 684, 10: 1323}\n",
      "------------------  0.4210522111052632 ------------------------\n",
      "exit rate per thresh  {1: 8944, 2: 18078, 3: 7255, 4: 3066, 5: 6128, 6: 3266, 7: 1083, 8: 925, 9: 466, 10: 789}\n",
      "------------------  0.4736837373684211 ------------------------\n",
      "exit rate per thresh  {1: 11666, 2: 19041, 3: 6831, 4: 2869, 5: 5109, 6: 2436, 7: 763, 8: 591, 9: 279, 10: 415}\n",
      "------------------  0.526315263631579 ------------------------\n",
      "exit rate per thresh  {1: 15020, 2: 19423, 3: 6176, 4: 2457, 5: 4005, 6: 1731, 7: 480, 8: 334, 9: 159, 10: 215}\n",
      "------------------  0.5789467898947368 ------------------------\n",
      "exit rate per thresh  {1: 19054, 2: 19049, 3: 5203, 4: 2002, 5: 2994, 6: 1116, 7: 257, 8: 160, 9: 76, 10: 89}\n",
      "------------------  0.6315783161578947 ------------------------\n",
      "exit rate per thresh  {1: 23560, 2: 17915, 3: 4198, 4: 1505, 5: 1974, 6: 612, 7: 101, 8: 65, 9: 34, 10: 36}\n",
      "------------------  0.6842098424210526 ------------------------\n",
      "exit rate per thresh  {1: 28508, 2: 16031, 3: 3108, 4: 962, 5: 1044, 6: 267, 7: 42, 8: 19, 9: 10, 10: 9}\n",
      "------------------  0.7368413686842105 ------------------------\n",
      "exit rate per thresh  {1: 33617, 2: 13337, 3: 1967, 4: 520, 5: 453, 6: 93, 7: 6, 8: 4, 9: 2, 10: 1}\n",
      "------------------  0.7894728949473684 ------------------------\n",
      "exit rate per thresh  {1: 38767, 2: 9821, 3: 1017, 4: 204, 5: 164, 6: 27}\n",
      "------------------  0.8421044212105263 ------------------------\n",
      "exit rate per thresh  {1: 43439, 2: 6083, 3: 394, 4: 55, 5: 29}\n",
      "------------------  0.8947359474736842 ------------------------\n",
      "exit rate per thresh  {1: 47056, 2: 2885, 3: 57, 4: 2}\n",
      "------------------  0.9473674737368422 ------------------------\n",
      "exit rate per thresh  {1: 49468, 2: 532}\n",
      "------------------  0.999999 ------------------------\n",
      "exit rate per thresh  {1: 50000}\n",
      "------------------------------------------------------------------\n",
      "exit rate average  [0.391538 0.213815 0.081094 0.035174 0.078104 0.053637 0.021061 0.020857\n",
      " 0.014318 0.090402]\n"
     ]
    }
   ],
   "source": [
    "##### passing TRAIN samples through model\n",
    "##### this is to save computation time and exit rates for all the possible branches\n",
    "print(\"%%%%%%%%%%%%%%%%%%%%%%% ResNet20 + CIFAR10(32x32)%%%%%%%%%%%%%%%%%%%%%%%%%%%%\")\n",
    "per_sample_label_list = []\n",
    "per_sample_out_vector_list = []\n",
    "per_sample_comp_latency_backbone_list = []\n",
    "per_sample_comp_latency_exitbranch_list = []\n",
    "\n",
    "\n",
    "#### saving intermediate data once\n",
    "for i in range (len(train_it)):\n",
    "    temp_batch = train_it[i]\n",
    "\n",
    "    for j in range (len(temp_batch[0])):\n",
    "        pic = temp_batch[0][j]\n",
    "        label = temp_batch[1][j]\n",
    "        per_sample_label_list.append(np.array(label).reshape(1,10))\n",
    "\n",
    "        res = model_CIFAR_ResNet(np.array(pic.reshape(1,image_resize,image_resize,3)), training = 1000)\n",
    "   \n",
    "        per_sample_out_vector_list.append(res[0])\n",
    "        per_sample_comp_latency_backbone_list.append(res[1])\n",
    "        per_sample_comp_latency_exitbranch_list.append(res[2])\n",
    "\n",
    "print (\"computation latencies backbone(train data)   \", np.mean(per_sample_comp_latency_backbone_list, axis=0)*1000)\n",
    "print (\"computation latencies exitbranch(train data)   \", np.mean(per_sample_comp_latency_exitbranch_list, axis=0)*1000)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "##### passing TRAIN samples through model\n",
    "##### this is to save computation time and exit rates for all the possible branches\n",
    "\n",
    "entropy_threshold_list = np.linspace(0.000000001, 0.999999, num=20)\n",
    "\n",
    "### all possible branches, and chosen number\n",
    "chosen_number = branch_number\n",
    "I = list(range(1, branch_number+1))\n",
    "\n",
    "#### number of train samples for simulation\n",
    "sample_number = train_it.samples\n",
    "\n",
    "\n",
    "\n",
    "#### per placement, in this case it is only 1, all the possible branches\n",
    "for item in combinations(I, chosen_number):\n",
    "    placement = list(np.array(item))\n",
    "    print (\"&&&&& selected exit \", placement , \"  &&&&&&\")\n",
    "        \n",
    "    ent_exitrate_list = []\n",
    "\n",
    "    ##### per threshold \n",
    "    for thresh in entropy_threshold_list:\n",
    "        print (\"------------------ \", thresh, \"------------------------\")\n",
    "\n",
    "        threshold_exit = []\n",
    "        threshold_exitrate = [[] for i in range(branch_number)]\n",
    "\n",
    "        ##### per sample\n",
    "        for sample in range (sample_number):\n",
    "\n",
    "            #### determining the exit branch based on entropy of the output and thresh\n",
    "            for exit in range(branch_number):\n",
    "                out = per_sample_out_vector_list[sample][exit]\n",
    "                entropy = -1 * tf.math.reduce_sum((tf.math.log(out) * out)/ np.log(out.shape[1]))\n",
    "\n",
    "                if (entropy < thresh or exit+1==branch_number):\n",
    "                    threshold_exit.append(exit+1)\n",
    "                    break\n",
    "\n",
    "\n",
    "        # handling exit percentage part\n",
    "        unique, counts = np.unique(threshold_exit, return_counts=True)\n",
    "        exitper_list_dict = dict(zip(unique, counts))\n",
    "        print (\"exit rate per thresh \", exitper_list_dict)\n",
    "\n",
    "        for i in range(branch_number):\n",
    "            if (exitper_list_dict.get(placement[i]) is None):\n",
    "                threshold_exitrate[i].append (0)\n",
    "            else:\n",
    "                threshold_exitrate[i].append(exitper_list_dict.get(placement[i]))\n",
    "\n",
    "\n",
    "        ent_exitrate_list.append(np.mean(threshold_exitrate, axis=1)/sample_number)\n",
    "\n",
    "    print(\"------------------------------------------------------------------\")\n",
    "    print(\"exit rate average \", np.mean(ent_exitrate_list, axis=0))\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "88be0fe0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "%%%%%%%%%%%%%%%%%%%%%%% ResNet20 + CIFAR10(128x128)%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "computation latencies backbone(train data)    [1.07179682 1.59485061 1.41119433 1.37436872 1.62892926 1.35556852\n",
      " 1.34866416 1.57074742 1.33205153 1.32506626]\n",
      "computation latencies exitbranch(train data)    [0.55097297 0.55931701 0.56123966 0.5415964  0.53471513 0.53009409\n",
      " 0.52753703 0.53294127 0.53218225 0.52879668]\n",
      "&&&&& selected exit  [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]   &&&&&&\n",
      "------------------  1e-09 ------------------------\n",
      "exit rate per thresh  {8: 7, 9: 133, 10: 49860}\n",
      "------------------  0.052631527263157896 ------------------------\n",
      "exit rate per thresh  {1: 2, 2: 1436, 3: 1498, 4: 1101, 5: 4443, 6: 4857, 7: 3187, 8: 8259, 9: 7077, 10: 18140}\n",
      "------------------  0.10526305352631579 ------------------------\n",
      "exit rate per thresh  {1: 8, 2: 2755, 3: 2718, 4: 1922, 5: 5653, 6: 5883, 7: 3525, 8: 8144, 9: 6246, 10: 13146}\n",
      "------------------  0.1578945797894737 ------------------------\n",
      "exit rate per thresh  {1: 21, 2: 4305, 3: 3899, 4: 2484, 5: 6337, 6: 6392, 7: 3600, 8: 7600, 9: 5582, 10: 9780}\n",
      "------------------  0.21052610605263158 ------------------------\n",
      "exit rate per thresh  {1: 50, 2: 6049, 3: 4968, 4: 2961, 5: 6845, 6: 6548, 7: 3632, 8: 6942, 9: 4794, 10: 7211}\n",
      "------------------  0.26315763231578954 ------------------------\n",
      "exit rate per thresh  {1: 96, 2: 8328, 3: 5842, 4: 3338, 5: 7117, 6: 6556, 7: 3558, 8: 6072, 9: 3997, 10: 5096}\n",
      "------------------  0.3157891585789474 ------------------------\n",
      "exit rate per thresh  {1: 260, 2: 10921, 3: 6841, 4: 3608, 5: 7105, 6: 6277, 7: 3260, 8: 5267, 9: 3094, 10: 3367}\n",
      "------------------  0.3684206848421053 ------------------------\n",
      "exit rate per thresh  {1: 721, 2: 13892, 3: 7609, 4: 3721, 5: 6886, 6: 5763, 7: 2776, 8: 4271, 9: 2248, 10: 2113}\n",
      "------------------  0.4210522111052632 ------------------------\n",
      "exit rate per thresh  {1: 1616, 2: 16643, 3: 8021, 4: 3741, 5: 6514, 6: 5068, 7: 2309, 8: 3291, 9: 1540, 10: 1257}\n",
      "------------------  0.4736837373684211 ------------------------\n",
      "exit rate per thresh  {1: 2866, 2: 19427, 3: 8121, 4: 3591, 5: 5846, 6: 4260, 7: 1798, 8: 2427, 9: 1005, 10: 659}\n",
      "------------------  0.526315263631579 ------------------------\n",
      "exit rate per thresh  {1: 4530, 2: 21938, 3: 8001, 4: 3373, 5: 4990, 6: 3416, 7: 1309, 8: 1579, 9: 545, 10: 319}\n",
      "------------------  0.5789467898947368 ------------------------\n",
      "exit rate per thresh  {1: 6824, 2: 24005, 3: 7504, 4: 3059, 5: 3937, 6: 2488, 7: 875, 8: 900, 9: 288, 10: 120}\n",
      "------------------  0.6315783161578947 ------------------------\n",
      "exit rate per thresh  {1: 9965, 2: 25138, 3: 6659, 4: 2560, 5: 2959, 6: 1583, 7: 527, 8: 456, 9: 115, 10: 38}\n",
      "------------------  0.6842098424210526 ------------------------\n",
      "exit rate per thresh  {1: 14332, 2: 25133, 3: 5388, 4: 1883, 5: 1933, 6: 870, 7: 238, 8: 176, 9: 38, 10: 9}\n",
      "------------------  0.7368413686842105 ------------------------\n",
      "exit rate per thresh  {1: 20476, 2: 22947, 3: 3833, 4: 1154, 5: 1031, 6: 405, 7: 85, 8: 51, 9: 15, 10: 3}\n",
      "------------------  0.7894728949473684 ------------------------\n",
      "exit rate per thresh  {1: 28625, 2: 17912, 3: 2331, 4: 577, 5: 403, 6: 117, 7: 23, 8: 11, 9: 1}\n",
      "------------------  0.8421044212105263 ------------------------\n",
      "exit rate per thresh  {1: 36754, 2: 11838, 3: 1073, 4: 202, 5: 111, 6: 19, 7: 1, 8: 2}\n",
      "------------------  0.8947359474736842 ------------------------\n",
      "exit rate per thresh  {1: 43443, 2: 6236, 3: 288, 4: 29, 5: 4}\n",
      "------------------  0.9473674737368422 ------------------------\n",
      "exit rate per thresh  {1: 48433, 2: 1550, 3: 17}\n",
      "------------------  0.999999 ------------------------\n",
      "exit rate per thresh  {1: 50000}\n",
      "------------------------------------------------------------------\n",
      "exit rate average  [0.269022 0.240453 0.084611 0.039304 0.072114 0.060502 0.030703 0.055455\n",
      " 0.036718 0.111118]\n"
     ]
    }
   ],
   "source": [
    "##### passing TRAIN samples through model\n",
    "##### this is to save computation time and exit rates for all the possible branches\n",
    "print(\"%%%%%%%%%%%%%%%%%%%%%%% ResNet20 + CIFAR10(128x128)%%%%%%%%%%%%%%%%%%%%%%%%%%%%\")\n",
    "per_sample_label_list = []\n",
    "per_sample_out_vector_list = []\n",
    "per_sample_comp_latency_backbone_list = []\n",
    "per_sample_comp_latency_exitbranch_list = []\n",
    "\n",
    "\n",
    "#### saving intermediate data once\n",
    "for i in range (len(train_it)):\n",
    "    temp_batch = train_it[i]\n",
    "\n",
    "    for j in range (len(temp_batch[0])):\n",
    "        pic = temp_batch[0][j]\n",
    "        label = temp_batch[1][j]\n",
    "        per_sample_label_list.append(np.array(label).reshape(1,10))\n",
    "\n",
    "        res = model_CIFAR_ResNet(np.array(pic.reshape(1,image_resize,image_resize,3)), training = 1000)\n",
    "   \n",
    "        per_sample_out_vector_list.append(res[0])\n",
    "        per_sample_comp_latency_backbone_list.append(res[1])\n",
    "        per_sample_comp_latency_exitbranch_list.append(res[2])\n",
    "\n",
    "print (\"computation latencies backbone(train data)   \", np.mean(per_sample_comp_latency_backbone_list, axis=0)*1000)\n",
    "print (\"computation latencies exitbranch(train data)   \", np.mean(per_sample_comp_latency_exitbranch_list, axis=0)*1000)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "##### passing TRAIN samples through model\n",
    "##### this is to save computation time and exit rates for all the possible branches\n",
    "\n",
    "entropy_threshold_list = np.linspace(0.000000001, 0.999999, num=20)\n",
    "\n",
    "### all possible branches, and chosen number\n",
    "chosen_number = branch_number\n",
    "I = list(range(1, branch_number+1))\n",
    "\n",
    "#### number of train samples for simulation\n",
    "sample_number = train_it.samples\n",
    "\n",
    "\n",
    "\n",
    "#### per placement, in this case it is only 1, all the possible branches\n",
    "for item in combinations(I, chosen_number):\n",
    "    placement = list(np.array(item))\n",
    "    print (\"&&&&& selected exit \", placement , \"  &&&&&&\")\n",
    "        \n",
    "    ent_exitrate_list = []\n",
    "\n",
    "    ##### per threshold \n",
    "    for thresh in entropy_threshold_list:\n",
    "        print (\"------------------ \", thresh, \"------------------------\")\n",
    "\n",
    "        threshold_exit = []\n",
    "        threshold_exitrate = [[] for i in range(branch_number)]\n",
    "\n",
    "        ##### per sample\n",
    "        for sample in range (sample_number):\n",
    "\n",
    "            #### determining the exit branch based on entropy of the output and thresh\n",
    "            for exit in range(branch_number):\n",
    "                out = per_sample_out_vector_list[sample][exit]\n",
    "                entropy = -1 * tf.math.reduce_sum((tf.math.log(out) * out)/ np.log(out.shape[1]))\n",
    "\n",
    "                if (entropy < thresh or exit+1==branch_number):\n",
    "                    threshold_exit.append(exit+1)\n",
    "                    break\n",
    "\n",
    "\n",
    "        # handling exit percentage part\n",
    "        unique, counts = np.unique(threshold_exit, return_counts=True)\n",
    "        exitper_list_dict = dict(zip(unique, counts))\n",
    "        print (\"exit rate per thresh \", exitper_list_dict)\n",
    "\n",
    "        for i in range(branch_number):\n",
    "            if (exitper_list_dict.get(placement[i]) is None):\n",
    "                threshold_exitrate[i].append (0)\n",
    "            else:\n",
    "                threshold_exitrate[i].append(exitper_list_dict.get(placement[i]))\n",
    "\n",
    "\n",
    "        ent_exitrate_list.append(np.mean(threshold_exitrate, axis=1)/sample_number)\n",
    "\n",
    "    print(\"------------------------------------------------------------------\")\n",
    "    print(\"exit rate average \", np.mean(ent_exitrate_list, axis=0))\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fdcae891",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "%%%%%%%%%%%%%%%%%%%%%%% ResNet44 + CIFAR10(32x32)%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "computation latencies backbone(train data)    [1.00379202 1.46268919 1.38500652 1.35506026 1.32993578 1.36724184\n",
      " 1.39032877 1.48021005 1.66795323 1.36200067 1.32484176 1.32581272\n",
      " 1.33411968 1.62183392 1.33781879 1.59764317 1.32429519 1.28260403\n",
      " 1.28274433 1.29705244 1.32905243 1.3020076 ]\n",
      "computation latencies exitbranch(train data)    [0.55167471 0.55648598 0.55566607 0.54259339 0.53706166 0.53820716\n",
      " 0.5387153  0.53937181 0.54460309 0.5381208  0.53403195 0.53667655\n",
      " 0.53930841 0.54038334 0.54030721 0.54171657 0.53493209 0.53356387\n",
      " 0.53759444 0.54038672 0.54095125 0.54117776]\n",
      "&&&&& selected exit  [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22]   &&&&&&\n",
      "------------------  1e-09 ------------------------\n",
      "exit rate per thresh  {8: 1, 10: 4, 11: 11, 12: 47, 13: 48, 14: 13, 15: 2, 16: 2, 17: 40, 18: 25, 19: 2, 20: 8, 22: 49797}\n",
      "------------------  0.052631527263157896 ------------------------\n",
      "exit rate per thresh  {1: 1, 2: 1736, 3: 3164, 4: 3850, 5: 3446, 6: 2940, 7: 2177, 8: 603, 9: 3278, 10: 5010, 11: 4248, 12: 2949, 13: 1728, 14: 1004, 15: 360, 16: 1177, 17: 1635, 18: 1079, 19: 509, 20: 225, 21: 131, 22: 8750}\n",
      "------------------  0.10526305352631579 ------------------------\n",
      "exit rate per thresh  {1: 4, 2: 3597, 3: 4781, 4: 4771, 5: 4015, 6: 3358, 7: 2547, 8: 741, 9: 3225, 10: 4768, 11: 3807, 12: 2542, 13: 1415, 14: 889, 15: 330, 16: 900, 17: 1242, 18: 815, 19: 374, 20: 176, 21: 90, 22: 5613}\n",
      "------------------  0.1578945797894737 ------------------------\n",
      "exit rate per thresh  {1: 11, 2: 5591, 3: 5965, 4: 5250, 5: 4357, 6: 3507, 7: 2742, 8: 718, 9: 3204, 10: 4355, 11: 3380, 12: 2157, 13: 1123, 14: 684, 15: 240, 16: 748, 17: 1016, 18: 589, 19: 292, 20: 154, 21: 73, 22: 3844}\n",
      "------------------  0.21052610605263158 ------------------------\n",
      "exit rate per thresh  {1: 46, 2: 7874, 3: 6874, 4: 5532, 5: 4582, 6: 3497, 7: 2751, 8: 800, 9: 2987, 10: 3875, 11: 3008, 12: 1723, 13: 918, 14: 556, 15: 207, 16: 582, 17: 754, 18: 470, 19: 230, 20: 118, 21: 72, 22: 2544}\n",
      "------------------  0.26315763231578954 ------------------------\n",
      "exit rate per thresh  {1: 157, 2: 10373, 3: 7502, 4: 5837, 5: 4726, 6: 3464, 7: 2626, 8: 777, 9: 2694, 10: 3382, 11: 2436, 12: 1457, 13: 717, 14: 392, 15: 166, 16: 437, 17: 567, 18: 347, 19: 182, 20: 87, 21: 49, 22: 1625}\n",
      "------------------  0.3157891585789474 ------------------------\n",
      "exit rate per thresh  {1: 671, 2: 13267, 3: 7948, 4: 5732, 5: 4745, 6: 3268, 7: 2347, 8: 698, 9: 2305, 10: 2897, 11: 1931, 12: 1101, 13: 566, 14: 286, 15: 114, 16: 305, 17: 380, 18: 248, 19: 115, 20: 77, 21: 42, 22: 957}\n",
      "------------------  0.3684206848421053 ------------------------\n",
      "exit rate per thresh  {1: 1932, 2: 15511, 3: 8185, 4: 5643, 5: 4527, 6: 3006, 7: 1965, 8: 591, 9: 2020, 10: 2360, 11: 1458, 12: 825, 13: 394, 14: 180, 15: 70, 16: 226, 17: 262, 18: 182, 19: 80, 20: 35, 21: 17, 22: 531}\n",
      "------------------  0.4210522111052632 ------------------------\n",
      "exit rate per thresh  {1: 3657, 2: 17463, 3: 8336, 4: 5347, 5: 4178, 6: 2554, 7: 1730, 8: 505, 9: 1669, 10: 1780, 11: 1022, 12: 579, 13: 232, 14: 108, 15: 47, 16: 141, 17: 194, 18: 98, 19: 50, 20: 27, 21: 13, 22: 270}\n",
      "------------------  0.4736837373684211 ------------------------\n",
      "exit rate per thresh  {1: 5842, 2: 19051, 3: 8150, 4: 5019, 5: 3649, 6: 2152, 7: 1396, 8: 416, 9: 1300, 10: 1284, 11: 710, 12: 345, 13: 142, 14: 80, 15: 40, 16: 94, 17: 108, 18: 55, 19: 20, 20: 8, 21: 8, 22: 131}\n",
      "------------------  0.526315263631579 ------------------------\n",
      "exit rate per thresh  {1: 8661, 2: 20285, 3: 7543, 4: 4496, 5: 3086, 6: 1778, 7: 1076, 8: 322, 9: 925, 10: 842, 11: 449, 12: 191, 13: 83, 14: 46, 15: 18, 16: 60, 17: 50, 18: 19, 19: 15, 20: 5, 22: 50}\n",
      "------------------  0.5789467898947368 ------------------------\n",
      "exit rate per thresh  {1: 12332, 2: 20815, 3: 6713, 4: 3924, 5: 2475, 6: 1263, 7: 664, 8: 228, 9: 612, 10: 505, 11: 240, 12: 93, 13: 39, 14: 25, 15: 6, 16: 16, 17: 18, 18: 6, 19: 8, 20: 2, 21: 1, 22: 15}\n",
      "------------------  0.6315783161578947 ------------------------\n",
      "exit rate per thresh  {1: 16933, 2: 20077, 3: 5971, 4: 3195, 5: 1767, 6: 764, 7: 408, 8: 121, 9: 319, 10: 264, 11: 93, 12: 42, 13: 11, 14: 7, 15: 3, 16: 9, 17: 2, 18: 7, 19: 1, 22: 6}\n",
      "------------------  0.6842098424210526 ------------------------\n",
      "exit rate per thresh  {1: 22091, 2: 18727, 3: 4912, 4: 2303, 5: 998, 6: 406, 7: 203, 8: 66, 9: 151, 10: 92, 11: 26, 12: 9, 13: 4, 14: 3, 15: 3, 16: 4, 18: 2}\n",
      "------------------  0.7368413686842105 ------------------------\n",
      "exit rate per thresh  {1: 27973, 2: 16359, 3: 3434, 4: 1373, 5: 498, 6: 182, 7: 73, 8: 17, 9: 45, 10: 32, 11: 11, 12: 1, 14: 1, 15: 1}\n",
      "------------------  0.7894728949473684 ------------------------\n",
      "exit rate per thresh  {1: 34228, 2: 12923, 3: 1938, 4: 640, 5: 192, 6: 38, 7: 16, 8: 7, 9: 11, 10: 5, 11: 1, 13: 1}\n",
      "------------------  0.8421044212105263 ------------------------\n",
      "exit rate per thresh  {1: 40147, 2: 8689, 3: 915, 4: 195, 5: 37, 6: 8, 7: 6, 8: 1, 9: 2}\n",
      "------------------  0.8947359474736842 ------------------------\n",
      "exit rate per thresh  {1: 45108, 2: 4634, 3: 224, 4: 27, 5: 6, 6: 1}\n",
      "------------------  0.9473674737368422 ------------------------\n",
      "exit rate per thresh  {1: 48706, 2: 1283, 3: 11}\n",
      "------------------  0.999999 ------------------------\n",
      "exit rate per thresh  {1: 50000}\n",
      "------------------------------------------------------------------\n",
      "exit rate average  [0.3185   0.218255 0.092566 0.063134 0.047284 0.032186 0.022727 0.006612\n",
      " 0.024747 0.031455 0.022831 0.014061 0.007421 0.004274 0.001607 0.004701\n",
      " 0.006268 0.003942 0.001878 0.000922 0.000496 0.074133]\n"
     ]
    }
   ],
   "source": [
    "##### passing TRAIN samples through model\n",
    "##### this is to save computation time and exit rates for all the possible branches\n",
    "print(\"%%%%%%%%%%%%%%%%%%%%%%% ResNet44 + CIFAR10(32x32)%%%%%%%%%%%%%%%%%%%%%%%%%%%%\")\n",
    "per_sample_label_list = []\n",
    "per_sample_out_vector_list = []\n",
    "per_sample_comp_latency_backbone_list = []\n",
    "per_sample_comp_latency_exitbranch_list = []\n",
    "\n",
    "\n",
    "#### saving intermediate data once\n",
    "for i in range (len(train_it)):\n",
    "    temp_batch = train_it[i]\n",
    "\n",
    "    for j in range (len(temp_batch[0])):\n",
    "        pic = temp_batch[0][j]\n",
    "        label = temp_batch[1][j]\n",
    "        per_sample_label_list.append(np.array(label).reshape(1,10))\n",
    "\n",
    "        res = model_CIFAR_ResNet(np.array(pic.reshape(1,image_resize,image_resize,3)), training = 1000)\n",
    "   \n",
    "        per_sample_out_vector_list.append(res[0])\n",
    "        per_sample_comp_latency_backbone_list.append(res[1])\n",
    "        per_sample_comp_latency_exitbranch_list.append(res[2])\n",
    "\n",
    "print (\"computation latencies backbone(train data)   \", np.mean(per_sample_comp_latency_backbone_list, axis=0)*1000)\n",
    "print (\"computation latencies exitbranch(train data)   \", np.mean(per_sample_comp_latency_exitbranch_list, axis=0)*1000)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "##### passing TRAIN samples through model\n",
    "##### this is to save computation time and exit rates for all the possible branches\n",
    "\n",
    "entropy_threshold_list = np.linspace(0.000000001, 0.999999, num=20)\n",
    "\n",
    "### all possible branches, and chosen number\n",
    "chosen_number = branch_number\n",
    "I = list(range(1, branch_number+1))\n",
    "\n",
    "#### number of train samples for simulation\n",
    "sample_number = train_it.samples\n",
    "\n",
    "\n",
    "\n",
    "#### per placement, in this case it is only 1, all the possible branches\n",
    "for item in combinations(I, chosen_number):\n",
    "    placement = list(np.array(item))\n",
    "    print (\"&&&&& selected exit \", placement , \"  &&&&&&\")\n",
    "        \n",
    "    ent_exitrate_list = []\n",
    "\n",
    "    ##### per threshold \n",
    "    for thresh in entropy_threshold_list:\n",
    "        print (\"------------------ \", thresh, \"------------------------\")\n",
    "\n",
    "        threshold_exit = []\n",
    "        threshold_exitrate = [[] for i in range(branch_number)]\n",
    "\n",
    "        ##### per sample\n",
    "        for sample in range (sample_number):\n",
    "\n",
    "            #### determining the exit branch based on entropy of the output and thresh\n",
    "            for exit in range(branch_number):\n",
    "                out = per_sample_out_vector_list[sample][exit]\n",
    "                entropy = -1 * tf.math.reduce_sum((tf.math.log(out) * out)/ np.log(out.shape[1]))\n",
    "\n",
    "                if (entropy < thresh or exit+1==branch_number):\n",
    "                    threshold_exit.append(exit+1)\n",
    "                    break\n",
    "\n",
    "\n",
    "        # handling exit percentage part\n",
    "        unique, counts = np.unique(threshold_exit, return_counts=True)\n",
    "        exitper_list_dict = dict(zip(unique, counts))\n",
    "        print (\"exit rate per thresh \", exitper_list_dict)\n",
    "\n",
    "        for i in range(branch_number):\n",
    "            if (exitper_list_dict.get(placement[i]) is None):\n",
    "                threshold_exitrate[i].append (0)\n",
    "            else:\n",
    "                threshold_exitrate[i].append(exitper_list_dict.get(placement[i]))\n",
    "\n",
    "\n",
    "        ent_exitrate_list.append(np.mean(threshold_exitrate, axis=1)/sample_number)\n",
    "\n",
    "    print(\"------------------------------------------------------------------\")\n",
    "    print(\"exit rate average \", np.mean(ent_exitrate_list, axis=0))\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5affcbae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "%%%%%%%%%%%%%%%%%%%%%%% ResNet110 + CIFAR10(32x32)%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "computation latencies backbone(train data)    [1.27908116 1.35492894 1.30633584 1.30529726 1.38806518 1.31773298\n",
      " 1.32200212 1.32281848 1.32391569 1.32322505 1.32270598 1.32323905\n",
      " 1.32360756 1.32472047 1.3233854  1.39121734 1.32444207 1.32335128\n",
      " 1.32470798 1.6305693  1.32882537 1.34499961 1.31871814 1.32252617\n",
      " 1.3328643  1.33493065 1.33015145 1.32807645 1.3267055  1.32853728\n",
      " 1.33130296 1.40780528 1.51088671 1.43084661 1.33068219 1.32731767\n",
      " 1.32729433 1.59393657 1.41442889 1.27291713 1.27760094 1.285372\n",
      " 1.28934536 1.3104925  1.2908643  1.28820662 1.28926319 1.29195539\n",
      " 1.37471923 1.45420452 1.28810961 1.29042663 1.28874732 1.28954314\n",
      " 1.28951315]\n",
      "computation latencies exitbranch(train data)    [0.5666563  0.53171494 0.52497724 0.52634132 0.53049194 0.53094642\n",
      " 0.53183299 0.53182475 0.53253701 0.53192595 0.53085081 0.53233934\n",
      " 0.53298107 0.53241436 0.53240119 0.533407   0.5331015  0.53127491\n",
      " 0.53243231 0.5373203  0.53096122 0.5271561  0.53023192 0.53129711\n",
      " 0.53202477 0.56288057 0.53398466 0.53317899 0.53235629 0.53326581\n",
      " 0.5345032  0.53419159 0.53366531 0.53337974 0.53322985 0.53194295\n",
      " 0.53259036 0.53525559 0.52997497 0.52758406 0.53179432 0.53554032\n",
      " 0.53558295 0.53618241 0.53669675 0.53607439 0.53664278 0.53671338\n",
      " 0.53523025 0.53639357 0.53495499 0.53558135 0.53566445 0.53688359\n",
      " 0.53621199]\n",
      "&&&&& selected exit  [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55]   &&&&&&\n",
      "------------------  1e-09 ------------------------\n",
      "exit rate per thresh  {13: 13, 14: 7, 15: 6, 20: 27, 21: 2, 22: 30, 23: 33, 24: 22, 25: 28, 26: 11, 27: 24, 28: 77, 29: 38, 30: 38, 31: 38, 32: 11, 33: 14, 34: 4, 35: 5, 36: 3, 38: 48, 39: 41, 40: 7, 41: 11, 42: 11, 43: 4, 44: 13, 45: 6, 46: 5, 47: 1, 49: 1, 50: 1, 52: 1, 55: 49419}\n",
      "------------------  0.052631527263157896 ------------------------\n",
      "exit rate per thresh  {2: 100, 3: 1859, 4: 1741, 5: 2195, 6: 3576, 7: 2483, 8: 1844, 9: 1903, 10: 1686, 11: 1891, 12: 1949, 13: 2054, 14: 1717, 15: 1065, 16: 752, 17: 483, 18: 251, 19: 151, 20: 1913, 21: 1349, 22: 2132, 23: 2106, 24: 1659, 25: 1381, 26: 1145, 27: 972, 28: 741, 29: 557, 30: 503, 31: 285, 32: 243, 33: 148, 34: 90, 35: 53, 36: 53, 37: 38, 38: 463, 39: 237, 40: 342, 41: 344, 42: 289, 43: 236, 44: 189, 45: 119, 46: 91, 47: 52, 48: 43, 49: 34, 50: 22, 51: 27, 52: 13, 53: 21, 54: 9, 55: 4401}\n",
      "------------------  0.10526305352631579 ------------------------\n",
      "exit rate per thresh  {1: 2, 2: 385, 3: 2985, 4: 3091, 5: 3671, 6: 4242, 7: 2742, 8: 2051, 9: 2115, 10: 1892, 11: 1943, 12: 2050, 13: 1891, 14: 1703, 15: 1116, 16: 642, 17: 460, 18: 243, 19: 170, 20: 1676, 21: 1214, 22: 1864, 23: 1724, 24: 1380, 25: 1117, 26: 821, 27: 705, 28: 542, 29: 413, 30: 361, 31: 202, 32: 162, 33: 74, 34: 67, 35: 43, 36: 32, 37: 39, 38: 308, 39: 175, 40: 233, 41: 259, 42: 217, 43: 183, 44: 100, 45: 80, 46: 59, 47: 39, 48: 25, 49: 15, 50: 15, 51: 7, 52: 12, 53: 7, 54: 8, 55: 2428}\n",
      "------------------  0.1578945797894737 ------------------------\n",
      "exit rate per thresh  {1: 14, 2: 912, 3: 4080, 4: 4340, 5: 4585, 6: 4466, 7: 2858, 8: 2210, 9: 2383, 10: 1959, 11: 1910, 12: 1961, 13: 1795, 14: 1492, 15: 958, 16: 602, 17: 372, 18: 209, 19: 181, 20: 1427, 21: 1060, 22: 1610, 23: 1418, 24: 1119, 25: 854, 26: 641, 27: 561, 28: 390, 29: 279, 30: 222, 31: 143, 32: 119, 33: 55, 34: 72, 35: 39, 36: 24, 37: 29, 38: 224, 39: 132, 40: 179, 41: 193, 42: 150, 43: 102, 44: 83, 45: 46, 46: 35, 47: 18, 48: 26, 49: 14, 50: 9, 51: 9, 52: 13, 53: 14, 54: 6, 55: 1398}\n",
      "------------------  0.21052610605263158 ------------------------\n",
      "exit rate per thresh  {1: 47, 2: 1756, 3: 5269, 4: 5416, 5: 5125, 6: 4517, 7: 3032, 8: 2287, 9: 2460, 10: 1910, 11: 1773, 12: 1823, 13: 1549, 14: 1326, 15: 853, 16: 530, 17: 346, 18: 168, 19: 168, 20: 1269, 21: 908, 22: 1293, 23: 1147, 24: 894, 25: 640, 26: 473, 27: 413, 28: 288, 29: 200, 30: 162, 31: 97, 32: 73, 33: 58, 34: 49, 35: 23, 36: 13, 37: 23, 38: 149, 39: 97, 40: 126, 41: 122, 42: 97, 43: 65, 44: 63, 45: 25, 46: 24, 47: 20, 48: 9, 49: 5, 50: 9, 51: 7, 52: 11, 53: 2, 54: 2, 55: 789}\n",
      "------------------  0.26315763231578954 ------------------------\n",
      "exit rate per thresh  {1: 147, 2: 3063, 3: 6466, 4: 6362, 5: 5463, 6: 4267, 7: 3096, 8: 2386, 9: 2389, 10: 1763, 11: 1592, 12: 1616, 13: 1349, 14: 1187, 15: 718, 16: 452, 17: 279, 18: 179, 19: 175, 20: 1014, 21: 734, 22: 1031, 23: 889, 24: 679, 25: 460, 26: 333, 27: 290, 28: 212, 29: 132, 30: 108, 31: 85, 32: 60, 33: 21, 34: 20, 35: 16, 36: 10, 37: 9, 38: 95, 39: 60, 40: 82, 41: 79, 42: 54, 43: 55, 44: 32, 45: 28, 46: 14, 47: 13, 48: 9, 49: 9, 50: 3, 51: 7, 52: 4, 53: 2, 54: 5, 55: 397}\n",
      "------------------  0.3157891585789474 ------------------------\n",
      "exit rate per thresh  {1: 434, 2: 5353, 3: 7398, 4: 7253, 5: 5230, 6: 3781, 7: 2952, 8: 2355, 9: 2278, 10: 1594, 11: 1372, 12: 1411, 13: 1181, 14: 923, 15: 618, 16: 339, 17: 242, 18: 143, 19: 125, 20: 815, 21: 560, 22: 813, 23: 691, 24: 452, 25: 316, 26: 230, 27: 190, 28: 106, 29: 92, 30: 77, 31: 52, 32: 31, 33: 12, 34: 21, 35: 5, 36: 4, 37: 4, 38: 75, 39: 37, 40: 52, 41: 46, 42: 38, 43: 24, 44: 22, 45: 11, 46: 14, 47: 8, 48: 5, 49: 2, 50: 1, 51: 7, 52: 5, 53: 2, 54: 1, 55: 197}\n",
      "------------------  0.3684206848421053 ------------------------\n",
      "exit rate per thresh  {1: 1059, 2: 7831, 3: 8113, 4: 7340, 5: 5078, 6: 3705, 7: 2828, 8: 2302, 9: 2030, 10: 1320, 11: 1137, 12: 1111, 13: 960, 14: 709, 15: 453, 16: 273, 17: 178, 18: 100, 19: 92, 20: 666, 21: 419, 22: 552, 23: 460, 24: 299, 25: 183, 26: 142, 27: 127, 28: 65, 29: 49, 30: 51, 31: 30, 32: 14, 33: 8, 34: 6, 35: 4, 36: 5, 37: 5, 38: 54, 39: 22, 40: 21, 41: 19, 42: 22, 43: 19, 44: 8, 45: 10, 46: 3, 47: 8, 48: 1, 49: 1, 51: 3, 52: 2, 53: 1, 55: 102}\n",
      "------------------  0.4210522111052632 ------------------------\n",
      "exit rate per thresh  {1: 2058, 2: 10139, 3: 8602, 4: 7425, 5: 4811, 6: 3559, 7: 2623, 8: 2051, 9: 1682, 10: 1070, 11: 915, 12: 906, 13: 716, 14: 520, 15: 354, 16: 186, 17: 119, 18: 74, 19: 69, 20: 444, 21: 303, 22: 377, 23: 303, 24: 182, 25: 97, 26: 89, 27: 59, 28: 36, 29: 31, 30: 18, 31: 12, 32: 7, 33: 5, 34: 4, 35: 3, 37: 4, 38: 27, 39: 9, 40: 13, 41: 14, 42: 12, 43: 9, 44: 7, 45: 3, 46: 4, 47: 1, 48: 1, 49: 2, 52: 1, 55: 44}\n",
      "------------------  0.4736837373684211 ------------------------\n",
      "exit rate per thresh  {1: 3480, 2: 12181, 3: 8975, 4: 7346, 5: 4506, 6: 3333, 7: 2253, 8: 1722, 9: 1319, 10: 856, 11: 673, 12: 664, 13: 518, 14: 367, 15: 206, 16: 156, 17: 88, 18: 38, 19: 56, 20: 290, 21: 211, 22: 244, 23: 180, 24: 94, 25: 54, 26: 40, 27: 36, 28: 20, 29: 16, 30: 5, 31: 3, 32: 2, 33: 3, 34: 5, 36: 1, 37: 1, 38: 13, 39: 5, 40: 4, 41: 5, 42: 3, 43: 3, 44: 3, 46: 2, 48: 2, 53: 2, 55: 16}\n",
      "------------------  0.526315263631579 ------------------------\n",
      "exit rate per thresh  {1: 5345, 2: 14179, 3: 9273, 4: 6990, 5: 3991, 6: 2884, 7: 1810, 8: 1371, 9: 982, 10: 649, 11: 509, 12: 458, 13: 331, 14: 242, 15: 136, 16: 82, 17: 62, 18: 29, 19: 31, 20: 174, 21: 124, 22: 119, 23: 84, 24: 38, 25: 28, 26: 22, 27: 14, 28: 6, 29: 4, 30: 2, 31: 2, 32: 2, 33: 1, 35: 2, 36: 1, 37: 1, 38: 2, 39: 4, 40: 1, 41: 4, 42: 1, 43: 2, 44: 1, 48: 1, 55: 6}\n",
      "------------------  0.5789467898947368 ------------------------\n",
      "exit rate per thresh  {1: 7762, 2: 16285, 3: 9091, 4: 6159, 5: 3457, 6: 2341, 7: 1378, 8: 1011, 9: 687, 10: 456, 11: 336, 12: 266, 13: 226, 14: 128, 15: 66, 16: 28, 17: 25, 18: 18, 19: 14, 20: 76, 21: 56, 22: 45, 23: 36, 24: 17, 25: 13, 26: 6, 27: 5, 29: 3, 30: 2, 31: 2, 38: 1, 40: 1, 41: 1, 42: 2}\n",
      "------------------  0.6315783161578947 ------------------------\n",
      "exit rate per thresh  {1: 10757, 2: 18280, 3: 8207, 4: 5300, 5: 2782, 6: 1769, 7: 934, 8: 670, 9: 437, 10: 225, 11: 193, 12: 136, 13: 103, 14: 44, 15: 27, 16: 11, 17: 5, 18: 7, 19: 9, 20: 37, 21: 22, 22: 20, 23: 16, 24: 4, 25: 2, 27: 1, 29: 1, 31: 1}\n",
      "------------------  0.6842098424210526 ------------------------\n",
      "exit rate per thresh  {1: 14266, 2: 19707, 3: 7128, 4: 4260, 5: 2048, 6: 1124, 7: 564, 8: 367, 9: 208, 10: 89, 11: 94, 12: 48, 13: 31, 14: 19, 15: 8, 16: 3, 17: 4, 19: 5, 20: 9, 21: 6, 22: 7, 23: 3, 24: 1, 25: 1}\n",
      "------------------  0.7368413686842105 ------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exit rate per thresh  {1: 19100, 2: 19756, 3: 5608, 4: 3138, 5: 1232, 6: 568, 7: 283, 8: 147, 9: 79, 10: 30, 11: 22, 12: 14, 13: 14, 14: 2, 15: 2, 16: 1, 17: 1, 19: 1, 21: 1, 24: 1}\n",
      "------------------  0.7894728949473684 ------------------------\n",
      "exit rate per thresh  {1: 26463, 2: 16759, 3: 3831, 4: 1906, 5: 591, 6: 263, 7: 103, 8: 51, 9: 15, 10: 7, 11: 4, 12: 6, 14: 1}\n",
      "------------------  0.8421044212105263 ------------------------\n",
      "exit rate per thresh  {1: 34948, 2: 11621, 3: 2248, 4: 895, 5: 189, 6: 64, 7: 20, 8: 9, 9: 3, 10: 2, 14: 1}\n",
      "------------------  0.8947359474736842 ------------------------\n",
      "exit rate per thresh  {1: 42395, 2: 6340, 3: 1015, 4: 216, 5: 28, 6: 6}\n",
      "------------------  0.9473674737368422 ------------------------\n",
      "exit rate per thresh  {1: 47807, 2: 2063, 3: 124, 4: 6}\n",
      "------------------  0.999999 ------------------------\n",
      "exit rate per thresh  {1: 50000}\n",
      "------------------------------------------------------------------\n",
      "exit rate average  [2.66084e-01 1.66710e-01 1.00272e-01 7.91840e-02 5.49820e-02 4.44650e-02\n",
      " 2.99590e-02 2.28340e-02 2.09700e-02 1.55080e-02 1.43640e-02 1.44190e-02\n",
      " 1.27310e-02 1.03880e-02 6.58600e-03 4.05700e-03 2.66400e-03 1.45900e-03\n",
      " 1.24700e-03 9.83700e-03 6.96900e-03 1.01370e-02 9.09000e-03 6.84100e-03\n",
      " 5.17400e-03 3.95300e-03 3.39700e-03 2.48300e-03 1.81500e-03 1.54900e-03\n",
      " 9.52000e-04 7.24000e-04 3.99000e-04 3.38000e-04 1.93000e-04 1.46000e-04\n",
      " 1.53000e-04 1.45900e-03 8.19000e-04 1.06100e-03 1.09700e-03 8.96000e-04\n",
      " 7.02000e-04 5.21000e-04 3.28000e-04 2.51000e-04 1.60000e-04 1.22000e-04\n",
      " 8.30000e-05 6.00000e-05 6.70000e-05 6.20000e-05 5.10000e-05 3.10000e-05\n",
      " 5.91970e-02]\n"
     ]
    }
   ],
   "source": [
    "##### passing TRAIN samples through model\n",
    "##### this is to save computation time and exit rates for all the possible branches\n",
    "print(\"%%%%%%%%%%%%%%%%%%%%%%% ResNet110 + CIFAR10(32x32)%%%%%%%%%%%%%%%%%%%%%%%%%%%%\")\n",
    "per_sample_label_list = []\n",
    "per_sample_out_vector_list = []\n",
    "per_sample_comp_latency_backbone_list = []\n",
    "per_sample_comp_latency_exitbranch_list = []\n",
    "\n",
    "\n",
    "#### saving intermediate data once\n",
    "for i in range (len(train_it)):\n",
    "    temp_batch = train_it[i]\n",
    "\n",
    "    for j in range (len(temp_batch[0])):\n",
    "        pic = temp_batch[0][j]\n",
    "        label = temp_batch[1][j]\n",
    "        per_sample_label_list.append(np.array(label).reshape(1,10))\n",
    "\n",
    "        res = model_CIFAR_ResNet(np.array(pic.reshape(1,image_resize,image_resize,3)), training = 1000)\n",
    "   \n",
    "        per_sample_out_vector_list.append(res[0])\n",
    "        per_sample_comp_latency_backbone_list.append(res[1])\n",
    "        per_sample_comp_latency_exitbranch_list.append(res[2])\n",
    "\n",
    "print (\"computation latencies backbone(train data)   \", np.mean(per_sample_comp_latency_backbone_list, axis=0)*1000)\n",
    "print (\"computation latencies exitbranch(train data)   \", np.mean(per_sample_comp_latency_exitbranch_list, axis=0)*1000)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "##### passing TRAIN samples through model\n",
    "##### this is to save computation time and exit rates for all the possible branches\n",
    "\n",
    "entropy_threshold_list = np.linspace(0.000000001, 0.999999, num=20)\n",
    "\n",
    "### all possible branches, and chosen number\n",
    "chosen_number = branch_number\n",
    "I = list(range(1, branch_number+1))\n",
    "\n",
    "#### number of train samples for simulation\n",
    "sample_number = train_it.samples\n",
    "\n",
    "\n",
    "\n",
    "#### per placement, in this case it is only 1, all the possible branches\n",
    "for item in combinations(I, chosen_number):\n",
    "    placement = list(np.array(item))\n",
    "    print (\"&&&&& selected exit \", placement , \"  &&&&&&\")\n",
    "        \n",
    "    ent_exitrate_list = []\n",
    "\n",
    "    ##### per threshold \n",
    "    for thresh in entropy_threshold_list:\n",
    "        print (\"------------------ \", thresh, \"------------------------\")\n",
    "\n",
    "        threshold_exit = []\n",
    "        threshold_exitrate = [[] for i in range(branch_number)]\n",
    "\n",
    "        ##### per sample\n",
    "        for sample in range (sample_number):\n",
    "\n",
    "            #### determining the exit branch based on entropy of the output and thresh\n",
    "            for exit in range(branch_number):\n",
    "                out = per_sample_out_vector_list[sample][exit]\n",
    "                entropy = -1 * tf.math.reduce_sum((tf.math.log(out) * out)/ np.log(out.shape[1]))\n",
    "\n",
    "                if (entropy < thresh or exit+1==branch_number):\n",
    "                    threshold_exit.append(exit+1)\n",
    "                    break\n",
    "\n",
    "\n",
    "        # handling exit percentage part\n",
    "        unique, counts = np.unique(threshold_exit, return_counts=True)\n",
    "        exitper_list_dict = dict(zip(unique, counts))\n",
    "        print (\"exit rate per thresh \", exitper_list_dict)\n",
    "\n",
    "        for i in range(branch_number):\n",
    "            if (exitper_list_dict.get(placement[i]) is None):\n",
    "                threshold_exitrate[i].append (0)\n",
    "            else:\n",
    "                threshold_exitrate[i].append(exitper_list_dict.get(placement[i]))\n",
    "\n",
    "\n",
    "        ent_exitrate_list.append(np.mean(threshold_exitrate, axis=1)/sample_number)\n",
    "\n",
    "    print(\"------------------------------------------------------------------\")\n",
    "    print(\"exit rate average \", np.mean(ent_exitrate_list, axis=0))\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "da2f234d",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### TEST PART, FOR SIMULATION ONLY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f730dc91",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### passing TEST samples through model once and save the intermediate data\n",
    "#### this is for doing the simulation\n",
    "\n",
    "per_sample_label_list = []\n",
    "per_sample_out_vector_list = []\n",
    "per_sample_comp_latency_backbone_list = []\n",
    "per_sample_comp_latency_exitbranch_list = []\n",
    "\n",
    "for i in range (len(test_it)):\n",
    "    temp_batch = test_it[i]\n",
    "\n",
    "    for j in range (len(temp_batch[0])):\n",
    "        pic = temp_batch[0][j]\n",
    "        label = temp_batch[1][j]\n",
    "        per_sample_label_list.append(np.array(label).reshape(1,10))\n",
    "\n",
    "        res = model_CIFAR_ResNet(np.array(pic.reshape(1,image_resize,image_resize,3)), training = 1000)\n",
    "   \n",
    "        per_sample_out_vector_list.append(res[0])\n",
    "        per_sample_comp_latency_backbone_list.append(res[1])\n",
    "        per_sample_comp_latency_exitbranch_list.append(res[2])\n",
    "        \n",
    "print (\"computation latencies backbone(test data)   \", np.mean(per_sample_comp_latency_backbone_list, axis=0)*1000)\n",
    "print (\"computation latencies exitbranch(test data)   \", np.mean(per_sample_comp_latency_exitbranch_list, axis=0)*1000)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
